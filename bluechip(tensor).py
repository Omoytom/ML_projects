# -*- coding: utf-8 -*-
"""Bluechip(tensor).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BUOOdGanDRY3fYBwDVBLq8sPrcLHW9gB
"""

!unzip bluechip-summit-credit-worthiness-prediction.zip

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
# %matplotlib inline
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split

db = pd.read_csv('/content/Train.csv')

X = db.drop("Loan_Status", axis = 1)
y = db["Loan_Status"]

np.random.seed(42)
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)

s = (X_train.dtypes == 'object')
object_cols = list(s[s].index)

from sklearn.preprocessing import OneHotEncoder

# Apply one-hot encoder to each column with categorical data
OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))
OH_cols_test = pd.DataFrame(OH_encoder.transform(X_test[object_cols]))

# One-hot encoding removed index; put it back
OH_cols_train.index = X_train.index
OH_cols_test.index = X_test.index

# Remove categorical columns (will replace with one-hot encoding)
num_X_train = X_train.drop(object_cols, axis=1)
num_X_test = X_test.drop(object_cols, axis=1)

# Add one-hot encoded columns to numerical features
OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)
OH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)

# Ensure all columns have string type
OH_X_train.columns = OH_X_train.columns.astype(str)
OH_X_test.columns = OH_X_test.columns.astype(str)

X_train = OH_X_train
X_test = OH_X_test

model = Sequential([


    Dense(units=16, activation = 'relu',input_shape = (450,),kernel_regularizer=tf.keras.regularizers.l2(0.03)),
   # Dense(units = 16, activation = 'relu'),
    Dense(units=8, activation = 'relu'),
    Dense(units = 4, activation = 'relu'),
    Dense(1, activation='sigmoid'),
    layers.BatchNormalization(),
])

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau

early_stopping = EarlyStopping(
    min_delta=0.001, # minimium amount of change to count as an improvement
    patience=20, # how many epochs to wait before stopping
    restore_best_weights=True,)

history = model.fit(X_train, y_train, epochs=100, batch_size=32,callbacks=[early_stopping], validation_split=0.2)

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()

# Show the plots
plt.tight_layout()
plt.show()

loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')

td = pd.read_csv('/content/Test.csv')

s = (td.dtypes == 'object')
categorical_columns = list(s[s].index)
X_new_encoded = pd.get_dummies(td, columns=categorical_columns)
X_new_encoded = X_new_encoded.reindex(columns=X_train.columns, fill_value=0)

predictions_prob = model.predict(X_new_encoded)

print(predictions_prob)

predictions = [1 if prob > 0.8 else 0 for prob in predictions_prob]

print(predictions)

td["Loan_Status"] = predictions
print(td[['ID', "Loan_Status"]])

data = td[['ID', "Loan_Status"]]
data.to_csv('submission_file.csv', index=False)